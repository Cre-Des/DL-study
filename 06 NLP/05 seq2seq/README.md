# Seq2seq
- 输入和输出均为序列（如词、字符或子词序列）。
- 输入与输出序列长度动态可变（例如翻译任务中，中英文句子长度可能不同）。

为了解决这类问题，研究者提出了Seq2Seq（Sequence to Sequence，序列到序列）模型。

## 模型结构

Seq2Seq 模型由一个**编码器**（Encoder）和一个**解码器**（Decoder）构成。

- 编码器负责提取输入序列的语义信息，并将其压缩为一个固定长度的上下文向量（Context Vector）；
- 解码器则基于该向量，逐步生成目标序列。

### 编码器
编码器主要由一个 **循环神经网络（RNN/LSTM/GRU）** 构成，其任务是将输入序列的语义信息提取并压缩为一个上下文向量。

在模型处理输入序列时，循环神经网络会依次接收每个token的输入，并在每个时间步步更新隐藏状态。每个隐藏状态都携带了截止到当前位置为止的信息。随着序列推进，信息不断累积，最终会在最后一个时间步形成一个包含整句信息的隐藏状态。

这个**最后的隐藏状态就会作为上下文向量**（context vector），传递给解码器，用于指导后续的序列生成。

为增强编码器的理解能力，循环网络也可以采用双向结构（结合前文与后文信息）或多层结构（提取更深的语义特征）

### 解码器

解码器主要也由一个 **循环神经网络（RNN / LSTM / GRU）** 构成，其任务是基于编码器传递的上下文向量，逐步生成目标序列。

在生成开始时，循环神经网络以上下文向量作为初始隐藏状态，并接收一个特殊的起始标记 `<sos>`（start of sentence）作为第一个时间步的输入，用于预测第一个 token。

随后，在每一个时间步，模型都会根据前一时刻的隐藏状态和上一步生成的 token，预测当前的输出。这种“将前一步的输出作为下一步输入”的方式被称为**自回归生成**（Autoregressive Generation），它确保了生成结果的连贯性。

生成过程会持续进行，直到模型生成了一个特殊的结束标记 `<eos>`（end of sentence），表示句子生成完成。

*说明：起始标记和结束标记会在训练数据中显式添加，模型会在训练中学会何时开始、如何续写，以及何时结束，从而掌握完整的生成流程。*

## 模型训练和推理机制
### 模型训练
Seq2Seq 模型的训练目标，是在给定输入序列的条件下，逐步生成完整且准确的目标序列。

1. **数据准备**  
为了让模型明确目标序列的起点和终点，通常在目标句前添加 `<sos>`（start of sequence），句末添加 `<eos>`（end of sequence）：  
“I like you.” → “<sos> I like you. <eos>”  
这两个特殊标记帮助模型学会从哪里开始生成，以及何时停止生成。

2. **前向传播**  
模型由编码器和解码器两部分组成：  
**（1）编码器**  
编码器接收源语言序列“我喜欢你。”，通过嵌入层和循环神经网络（RNN / LSTM / GRU）的逐步处理，将整句编码为上下文向量。  
**（2）解码器**  
解码器使用该上下文向量初始化其隐藏状态，然后逐步生成目标序列。
需要特别注意的是，训练阶段与推理阶段的解码策略是不同的：
在推理阶段，解码器采用**自回归生成方式**：每一步的输入是模型自己上一步的预测结果。  
而在训练阶段，通常使用一种称为 **Teacher Forcing** 的策略，即：
解码器每一步的输入不是模型上一步的预测结果，而是目标序列中真实的前一个token。
这种做法带来了两个明显好处：
   - 训练更快，误差不会累积；
   - 梯度传播更稳定，有利于优化收敛。
3. **计算损失**  
解码器每一步输出一个token的概率分布，我们通过**交叉熵损失函数**衡量模型对真实词的预测质量。训练过程中，每一个时间步都会产生一个损失值。该样本的总损失，就是所有时间步的损失值逐步累加的结果。
4. **反向传播**  
在 PyTorch 中，调用 `loss.backward()` 即可自动完成梯度的反向传播。系统会沿时间维度展开计算图，自动完成所有参数的梯度计算，无需手动推导，实现简洁高效。

### 模型推理

模型推理是Seq2Seq模型在实际任务中生成目标序列的过程，通常包括以下几个环节：

1. **编码器处理**  
推理阶段的编码器处理流程与训练时完全一致。
输入序列会经过分词、嵌入和循环神经网络的逐步处理，最终生成一个表示整句语义的上下文向量，该向量将作为解码器的初始隐藏状态，为生成过程提供语义基础。
2. **解码器处理**  
解码器是推理过程的核心，其生成方式采用**自回归生成**（Autoregressive Generation）：每一步的输出会作为下一步的输入，逐步构造完整句子。  
**（1）自回归生成流程**  
第一步，解码器接收起始标记 <sos>，生成第一个词；  
第二步，将上一步生成的词作为当前输入，再预测第二个词；  
持续重复以上过程，直到模型生成 <eos>，或达到设定的最大生成步数。  
**（2）词选择策略**  
每个时间步，解码器输出的是一个词概率分布。我们需要从中选择一个具体词作为本时间步的输出，选择方式即为生成策略。常见策略包括：  
   - **贪心解码**（Greedy Decoding）
每一步都选择概率最高的词。
优点：简单高效
缺点：容易陷入局部最优，生成不够多样。
   - **束搜索**（Beam Search）
每一步保留多个候选词序列（如 beam size = 3），并在扩展后选择得分最高的完整句子。
优点：全局考虑，生成质量高
缺点：计算开销大

## 存在问题
在上述 Seq2Seq 架构中，编码器会将整个源句压缩为一个固定长度的上下文向量，并将其作为解码器生成目标序列的唯一参考。这种“压缩再解压”的方式虽然结构简洁，但在实际任务中暴露出两个核心问题：

1. **信息压缩困难，语义表达受限**  
对于编码器而言，用一个定长向量去表达任意复杂的句子，是一项非常困难的任务。尤其在面对长句时，信息很容易在压缩过程中丢失，导致语义表达不完整。  
这种“信息瓶颈”限制了模型在处理长文本或复杂语义结构时的表现。
2. **缺乏动态感知，解码难以精准生成**  
解码器始终只能基于同一个上下文向量进行生成。
但在实际生成过程中，不同位置的目标词，往往依赖源句中不同的关键信息：
生成主语时，可能更依赖源句的开头；
生成谓语或宾语时，可能需要参考句中或句末内容。
然而在固定表示下，解码器无法“有选择地关注”输入序列的不同部分，只能一视同仁地处理所有信息，从而降低了生成的准确性与灵活性。
